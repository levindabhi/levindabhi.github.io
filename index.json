<<<<<<< HEAD
[{"authors":["admin"],"categories":null,"content":"Hello, I\u0026rsquo;m a final year Computer Science Undergraduate working towards developing robust and production-ready solutions of Computer Vision problems. Apart from my day job as a computer vision intern I work on generative modelling during night time.\nI am most skilled at Python, Python libraries like Numpy, Sklearn, OpenCV, Pillow, Matplotlib, Deep Learning framework PyTorch. I have experience with technology like C, C++, Java, DL framework TensorFlow and Keras, HTML, CSS, Web framework Flask, SQL, Google Cloud Platform, AWS and Azure.\nI like solving a mathematics puzzle, playing-watching video games, and playing-watching Garba apart from above things. If you are interested to know more about me, want to discuss any new idea, or want to discuss any ongoing esports tournament say hello on Twitter.\nI am open to a full-time position as a Machine Learning engineer/researcher. If you are startup solving challenging of computer vision and interested in recruiting me please reach out to me through email.\n","date":1602460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1602460800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://levindabhi.github.io/author/levin-dabhi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/levin-dabhi/","section":"authors","summary":"Hello, I\u0026rsquo;m a final year Computer Science Undergraduate working towards developing robust and production-ready solutions of Computer Vision problems. Apart from my day job as a computer vision intern I work on generative modelling during night time.","tags":null,"title":"Levin Dabhi","type":"authors"},{"authors":["Levin Dabhi"],"categories":null,"content":"Blending of different models in stylegan was first introduced by Justin Pinkney in his post: StyleGAN network blending. The fundamental idea he proposed was was swapping layers between two models. A base model trained on any dataset is blended with another model which is fine-tuned on the base model. For blending, few layers from the base model are swapped with the same layers of the fine-tuned model or vice-versa.\nThis lets one control what type of features one want from each model. Here in the post Ukiyo-e Yourself, Justin swapped higher resolution layers from stylegan2 trained on FFHQ dataset with higher resolution layers from model fine-tuned on Ukiyo-e photos. Here low-level features like pose, eyes, nose and other details are from FFHQ model and high-level features like texture, skin colour is from a fine-tuned model.\nAfter Justin open-sourced his colab notebook to blend different model of stylegan2, people tried many different styles and got interesting results. Here are some of the blended models details with few results.\nPainting  Dataset: MetFaces Fine-tuned by: AK  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 128x128, 32x32 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on Met Faces.\n Below are some of the cherry pick results.       Cartoon  Dataset: Cartoon images Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on cartoon images.\n Below are some of the cherry pick results.       Comic face  Dataset: Comic and Monster faces Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on comic images.\n Below are some of the cherry pick results.       WikiArt  Dataset: WikiArt Fine-tuned by: Peter Baylies  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 128x128, 64x64 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on wikiArt images.\n Below are some of the cherry pick results.       Monster Face  Dataset: WoWFaces Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 8x8 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on WOWFaces images.\n Below are some of the cherry pick results.       Furby Toys  Dataset: Furby Images Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 256x256, 64x64 and 8x8 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on furby toy images.\n Below are some of the cherry pick results.       Will be updating this post once I fine-tune on more datasets. Have a look after a few days. Some cool results   Interpolation of few AI/ML researchers into different styles   \r\r Projected image of each resarcher into FFHQ latent space to find the closest latent vector. Obtained vector is then inputed into a blended model of the style domain.\n  Interpolation of few actors and actress into different styles   \r\r Projected image of each actor/actress into FFHQ latent space to find the closest latent vector. Obtained vector is then inputed into a blended model of the style domain.\n  Exploration of cartoon latent space with music   This looks so amazing. Visualizing @Norod78 and @Buntworthy \u0026#39;s #toonify with music effect. Sync in the middle is so perfect. (Turn on sound if you haven\u0026#39;t ;)) pic.twitter.com/UL4H3RlVZi\n\u0026mdash; Levin Dabhi (@DabhiLevin) October 9, 2020  Used \u0026lsquo;Culture Shock\u0026rsquo; stylegan visualizer with some modifications.\nUpdates Out on arxiv, submitted to neurips creativity workshop (cc @elluba), work done with @Norod78 https://t.co/afkOiAXj95\n\u0026mdash; Justin (@Buntworthy) October 13, 2020  Justin and Doron submitted this idea at Neurips creativity workshop.\n 🐦Thank you for reading, if you want to comment or have any suggestions feel free to ping me on twitter.  ","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602460800,"objectID":"2979a9142fa6f3109a75d68b13daadd1","permalink":"https://levindabhi.github.io/post/generating-different-styles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/generating-different-styles/","section":"post","summary":"Results of blending various models in stylegan/stylegan2","tags":null,"title":"Generating different styles from stylegan","type":"post"}]
=======
[{"authors":["admin"],"categories":null,"content":"Hello, I\u0026rsquo;m a final year Computer Science Undergraduate at Nirma University working towards solving real-world problems by Machine learning, Deep learning, and software development. I have worked on projects from different domains including Computer Vision, Time Series Data, Robotics, Industrial machinery, and Sports analysis during my academics and internships.\nI am most skilled at Python, Python libraries like Numpy, Sklearn, OpenCV, Pillow, Matplotlib, Deep Learning framework PyTorch. I have experience with technology like C, C++, Java, DL framework TensorFlow and Keras, HTML, CSS, Web framework Flask, SQL, Google Cloud Platform, AWS for Deep Learning.\nApart from above mention things I like solving mathematics puzzle, playing-watching video games, and playing-watching Garba.\nIf you are interested to know more about me, want to discuss any new idea, or want to discuss any ongoing esports tournament say hello on twitter.\n","date":1602460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1602460800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://levindabhi.github.io/author/levin-dabhi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/levin-dabhi/","section":"authors","summary":"Hello, I\u0026rsquo;m a final year Computer Science Undergraduate at Nirma University working towards solving real-world problems by Machine learning, Deep learning, and software development. I have worked on projects from different domains including Computer Vision, Time Series Data, Robotics, Industrial machinery, and Sports analysis during my academics and internships.","tags":null,"title":"Levin Dabhi","type":"authors"},{"authors":["Levin Dabhi"],"categories":null,"content":"Blending of different models in stylegan was introduced by Justin Pinkney in his post: StyleGAN network blending. The basic idea proposed in by him is swapping layers between two models. A base model trained on any dataset is blended with another model which is fine-tuned on the base model. For blending, few layers from the base model are swapped with the same layers of the fine-tuned model or vice-versa.\nThis lets you control what type of features you want from each model. Here in post Ukiyo-e Yourself, Justin swapped higher resolution layers from stylegan2 trained on FFHQ dataset with higher resolution layers from model fine-tuned on Ukiyo-e photos. Here low-level features like pose, eyes, nose and other details are from FFHQ model and high-level features like texture, skin colour is from a fine-tuned model.\nAfter Justin open-sourced his colab notebook to blend different model of stylegan2, people tried many different styles and got interesting results. Here are some of the blended models details with few results.\nPainting  Dataset: MetFaces Fine-tuned by: AK  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 128x128, 32x32 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on Met Faces.\n Below are some of the cherry pick results.       Cartoon  Dataset: Cartoon images Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on cartoon images.\n Below are some of the cherry pick results.       Comic face  Dataset: Comic and Monster faces Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on comic images.\n Below are some of the cherry pick results.       WikiArt  Dataset: WikiArt Fine-tuned by: Peter Baylies  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 128x128, 64x64 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on wikiArt images.\n Below are some of the cherry pick results.       Monster Face  Dataset: WoWFaces Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 8x8 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on WOWFaces images.\n Below are some of the cherry pick results.       Furby Toys  Dataset: Furby Images Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 256x256, 64x64 and 8x8 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on furby toy images.\n Below are some of the cherry pick results.       I will be adding a few more styles when I will get time and resources for fine-tuning on more datasets. Have a look after a few days. Some cool results   Interpolation of few AI/ML researchers into different styles   \r\r Projected image of each resarcher into FFHQ latent space to find the closest latent vector. Obtained vector is then inputed into a blended model of the style domain.\n  Interpolation of few actors and actress into different styles   \r\r Projected image of each actor/actress into FFHQ latent space to find the closest latent vector. Obtained vector is then inputed into a blended model of the style domain.\n  Exploration of cartoon latent space with music   This looks so amazing. Visualizing @Norod78 and @Buntworthy \u0026#39;s #toonify with music effect. Sync in the middle is so perfect. (Turn on sound if you haven\u0026#39;t ;)) pic.twitter.com/UL4H3RlVZi\n\u0026mdash; Levin Dabhi (@DabhiLevin) October 9, 2020  Used \u0026lsquo;Culture Shock\u0026rsquo; stylegan visualizer with some modifications.\n 🐦Thank you for reading, if you want to comment or have any suggestions feel free to ping me on twitter.  ","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602460800,"objectID":"2979a9142fa6f3109a75d68b13daadd1","permalink":"https://levindabhi.github.io/post/generating-different-styles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/generating-different-styles/","section":"post","summary":"Results of blending various models in stylegan/stylegan2","tags":null,"title":"Generating different styles from stylegan","type":"post"}]
>>>>>>> 6a44d793a3b04d439be05215ac61a85c4e3f63a4
