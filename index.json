[{"authors":["admin"],"categories":null,"content":"Hello, I‚Äôm a Machine Learning Engineer working at Ceremorphic Inc in the compiler development team. Our team works on the R\u0026amp;D of Machine Learning compilers for low power AI accelerators. Prior to this, during my academics, I worked on research problems in the area of image and video generation. Apart from my day job as a Machine Learning Engineer, I like to play with generative models.\nI am most skilled at Python, C++ and deep learning framework PyTorch. I have experience with technology like C, Java, DL framework Jax, TensorFlow and Keras, HTML, CSS, Web framework Flask, SQL, Google Cloud Platform, AWS and Azure.\nI like solving a mathematics puzzle, playing-watching video games, and playing-watching Garba apart from above things. If you are interested to know more about me, want to discuss any of my work, want to discuss any new idea, or want to discuss any ongoing esports tournament say hello on Twitter.\n","date":1612715198,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1612715198,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://levindabhi.github.io/author/levin-dabhi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/levin-dabhi/","section":"authors","summary":"Hello, I‚Äôm a Machine Learning Engineer working at Ceremorphic Inc in the compiler development team. Our team works on the R\u0026amp;D of Machine Learning compilers for low power AI accelerators. Prior to this, during my academics, I worked on research problems in the area of image and video generation.","tags":null,"title":"Levin Dabhi","type":"authors"},{"authors":["Levin Dabhi"],"categories":null,"content":"After one of my tweet went semi-viral, I motivated myself to put together the results of my stylegan2-ADA experiment in this post. Addition of augmentation in the discriminator of stylegan2 proved to be the cherry on the cake if one wants to train stylegan2 for a limited number of images. In the StyleganADA paper authors explore small datasets with around 1500-2000 images. I took up the task of finding the lower limit and train on a few hundred images.\nI trained on three categories namely teddy bear faces, AmongUs characters and Lego images. I used the original stylegan2-ADA repo (TensorFlow version) for training. Main points that make the training successful without mode collapse are\n Right Initial point Fine-tuning from weight file which is close to your data helps a lot Augmentation Strength If your dataset contains a few hundred images it is better to start with high augment strength and keep target low to speedily ramp up the augment strength. Learning rate and Gamma Its better to explore few values Learning rate and Gamma before starting full training. Also, it is recommended to try different values when mode collapse occurs.  Below are details of a dataset, weight file link and results of each category. Hope you have a fun time watching the results!\nTeddy Bear face  Dataset: Teddy bear dataset No of Images: 637 pkl file link: google drive  \r\r  Randomly picked samples   AmongUS Characters  Dataset: Scrapped from citypng.com No of Images: 203 pkl file link: google drive  \r\r  Randomly picked samples     Style Mixing examples   \r\rTurn on audio üîäüéß for best experience. Above video is generated from Culture Shock with modifications. Music is from this youtube video.\nLego images  Dataset: Scrapped from brickset.com No of Images: 31 pkl file link: google drive  Failure case: As the number of images are very low and diversity among them is very high, despite trying above mention steps it failed to converge properly.\n\r\r  Randomly picked samples    üê¶Thank you for reading, if you want to comment or have any suggestions feel free to ping me on twitter.  ","date":1612715198,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612715198,"objectID":"bed1140cb8f5cdc91402b15692e60326","permalink":"https://levindabhi.github.io/post/generating-amongus-characters/","publishdate":"2021-02-07T21:56:38+05:30","relpermalink":"/post/generating-amongus-characters/","section":"post","summary":"Results of training stylegan2-ADA with few images","tags":null,"title":"Generating Amongus Characters","type":"post"},{"authors":["Levin Dabhi"],"categories":null,"content":"Blending of different models in stylegan was first introduced by Justin Pinkney in his post: StyleGAN network blending. The fundamental idea he proposed was swapping layers between two models. A base model trained on any dataset is blended with another model which is fine-tuned on the base model. For blending, few layers from the base model are swapped with the same layers of the fine-tuned model or vice-versa.\nThis lets one control what type of features one want from each model. Here in the post Ukiyo-e Yourself, Justin swapped higher resolution layers from stylegan2 trained on FFHQ dataset with higher resolution layers from model fine-tuned on Ukiyo-e photos. Here low-level features like pose, eyes, nose and other details are from FFHQ model and high-level features like texture, skin colour is from a fine-tuned model.\nAfter Justin open-sourced his colab notebook to blend different model of stylegan2, people tried many different styles and got interesting results. Here are some of the blended models details with few results.\nPainting  Dataset: MetFaces Fine-tuned by: AK  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 128x128, 32x32 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on Met Faces.\n Below are some of the cherry pick results.       Cartoon  Dataset: Cartoon images Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on cartoon images.\n Below are some of the cherry pick results.       Comic face  Dataset: Comic and Monster faces Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on comic images.\n Below are some of the cherry pick results.       WikiArt  Dataset: WikiArt Fine-tuned by: Peter Baylies  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 128x128, 64x64 and 4x4 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on wikiArt images.\n Below are some of the cherry pick results.       Monster Face  Dataset: WoWFaces Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 64x64, 16x16 and 8x8 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on WOWFaces images.\n Below are some of the cherry pick results.       Furby Toys  Dataset: Furby Images Fine-tuned by: Doron Adler  \r\r\r Here is output of model blended from different resolutions.   First one is from FFHQ model, followed by model blended from 256x256, 64x64 and 8x8 resolution   Lower resolutions layer are taken from model trained on FFHQ while higher resolution layers are from model fine-tuned on furby toy images.\n Below are some of the cherry pick results.       Will be updating this post once I fine-tune on more datasets. Have a look after a few days. Some cool results   Interpolation of few AI/ML researchers into different styles   \r\r Projected image of each resarcher into FFHQ latent space to find the closest latent vector. Obtained vector is then inputed into a blended model of the style domain.\n  Interpolation of few actors and actress into different styles   \r\r Projected image of each actor/actress into FFHQ latent space to find the closest latent vector. Obtained vector is then inputed into a blended model of the style domain.\n  Exploration of cartoon latent space with music   This looks so amazing. Visualizing @Norod78 and @Buntworthy \u0026#39;s #toonify with music effect. Sync in the middle is so perfect. (Turn on sound if you haven\u0026#39;t ;)) pic.twitter.com/UL4H3RlVZi\n\u0026mdash; Levin (@DabhiLevin) October 9, 2020  Used \u0026lsquo;Culture Shock\u0026rsquo; stylegan visualizer with some modifications.\nUpdates Out on arxiv, submitted to neurips creativity workshop (cc @elluba), work done with @Norod78 https://t.co/afkOiAXj95\n\u0026mdash; Justin (@Buntworthy) October 13, 2020  Justin and Doron submitted this idea at Neurips creativity workshop.\n üê¶Thank you for reading, if you want to comment or have any suggestions feel free to ping me on twitter.  ","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602460800,"objectID":"2979a9142fa6f3109a75d68b13daadd1","permalink":"https://levindabhi.github.io/post/generating-different-styles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/generating-different-styles/","section":"post","summary":"Results of blending various models in stylegan/stylegan2","tags":null,"title":"Generating different styles from stylegan","type":"post"}]